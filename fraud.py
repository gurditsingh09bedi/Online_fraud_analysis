# -*- coding: utf-8 -*-
"""fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q7yTiGxSUh1cPAUS-u149FDXZulCTyek
"""

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import classification_report,accuracy_score
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

#load dataset
data = pd.read_csv("frauddata.csv")

# Taking half data.
shuffled_data = data.sample(frac=1, random_state=42)
dataset = shuffled_data.head(200000)

# Checking if there is any missing values in row wise
# Sorting misisng values in rows in decending order
dataset.isnull().sum(axis=1).sort_values(ascending=False)

#Checking if there are any missing values in rows
dataset.isnull().any(axis=1)

#Checking if the row having missing values grater than 100
dataset[dataset.isnull().sum(axis=1) > 100]

# Checking if there is any missing values columns wise
# Checking  any missing values in columns

x = dataset.isnull().sum()
y = (dataset.isnull().sum()/dataset.shape[0])*100
z = {'Total number of missing values':x,'Percentage of missing values':y}
df = pd.DataFrame(z,columns=['Total number of missing values','Percentage of missing values'])
df.sort_values(by='Percentage of missing values', ascending=True)
print(df)

# Drop the columns which is not relevent in machine learning model.
dataset = dataset.drop(['step'],axis=1)

#know check specific columns is droppped or not
dataset.columns

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
dataset['type']= le.fit_transform(dataset['type'])
dataset['type'].unique()

dataset.columns

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#features for analysis
selected_features = ['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig',
                      'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']

# Extract the selected features from the dataset
selected_df = dataset[selected_features]

# Calculate the correlation matrix
correlation_matrix = selected_df.corr()

# Create a heatmap for the correlation matrix
plt.figure(figsize=(14, 7))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)

# Set labels and title
plt.title('Online Fraud Dataset - Correlation Matrix Heatmap')

# Show the plot
plt.show()

# Outliers ckeck

plt.rcParams['figure.figsize'] = (16,6)
plt.subplot(2,3,1)
sns.boxplot(data=dataset, x='type')
plt.title('type')

plt.subplot(2,3,2)
sns.boxplot(data=dataset, x='isFraud')
plt.title('isFraud')

plt.subplot(2,3,3)
sns.boxplot(data=dataset, x='isFlaggedFraud')
plt.title('isFlaggedFraud')

plt.suptitle("Outliers Univarite")
plt.show()

#Removing outliers
dataset['isFraud'].values[dataset['isFraud'].values>1] = 1

# create a new varibale
dataset['encoded'] = le.inverse_transform(dataset['type'])
print(f"Decoded categories: {dataset['type']}")

#Create dfcol to represent values
dfcol = pd.DataFrame(dataset)
dfcol[['encoded','type']]

# Drop irrevelents values
dataset = dataset.drop(['nameOrig','nameDest','encoded'],axis=1)

# After dropped check if there is any missing values
x = dataset.isnull().sum()
y = (dataset.isnull().sum()/dataset.shape[0])*100
z = {'Total number of missing values':x,'Percentage of missing values':y}
df = pd.DataFrame(z,columns=['Total number of missing values','Percentage of missing values'])
df.sort_values(by='Percentage of missing values', ascending=True)
print(df)

# Case 1 Predict a transaction whether it is fraudulent or not based on the features, not every transaction fraudulent. To identify this classification model is used
# Divide the dataset in x and y variable
x_values = dataset.drop('isFraud',axis=1)
y_values = dataset['isFraud']

x_values.head

y_values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_values,y_values, test_size=0.4, random_state=1)

#create a list which store the avg score of cross-validation for each values of K

avg_knn_scroe =[]

# know taking value of take k starting from 1 to user  values

user_input_k = int(input("Enter the end value of K starting from 1 to your input value = "))
knn_values_list = list(range(1,user_input_k))

for i in knn_values_list:
    knn = KNeighborsClassifier(n_neighbors=i)
    scroes_knn = cross_val_score(knn,x_train,y_train, cv=10, scoring='accuracy')
    avg_knn_scroe.append(scroes_knn.mean())
print(avg_knn_scroe)

plt.rcParams['figure.figsize']=(16,6)
plt.plot(knn_values_list, avg_knn_scroe,marker='*')
plt.xlabel('Values of K for KNN')
plt.ylabel('Cross-Validation on Heart Desiase Dataset')
best_k = knn_values_list[avg_knn_scroe.index(max(avg_knn_scroe))]
plt.show(),best_k
print(plt.show(),best_k)

knn.fit(x_train,y_train)
y_pred=knn.predict((x_test))

#confuse matrix
cn = confusion_matrix(y_pred,y_test)
sns.heatmap(cn, annot=True)
plt.xlabel('Predicted values by the classifier')
plt.ylabel('Represents the true values')

print(accuracy_score(y_pred, y_test)*100)
print(classification_report(y_pred,y_test))



# Case 2 K-means Clustering Clustering: Group transactions into clusters based on their transaction characteristics such as amount, oldbalanceOrg, newbalanceOrig, oldbalanceDest, and newbalanceDest as features that will help on to check whether a person received Flagged Fraud or not.
x_k = dataset.iloc[:,:-1]
y_k = dataset.iloc[:,-1]

y_k.head

#creating clustering model to check the stable values of K
from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
  kmean = KMeans(n_clusters=i, init='k-means++', random_state=42)
  kmean.fit(x_train)
  wcss.append(kmean.inertia_)
print(wcss)
plt.plot(range(1,11),wcss,marker='o')
plt.title('The Elbow Method')
plt.xlabel('Number of Cluster')
plt.ylabel('WCSS')
plt.show()

kmeansdata = dataset.drop('isFraud',axis=1)

# Check missing values after dropped isFraud column
x = kmeansdata.isnull().sum()
y = (kmeansdata.isnull().sum()/kmeansdata.shape[0])*100
z = {'Total number of missing values':x,'Percentage of missing values':y}
df = pd.DataFrame(z,columns=['Total number of missing values','Percentage of missing values'])
df.sort_values(by='Percentage of missing values', ascending=True)
print(df)

# Divide the dataset for clustering
x_kmeans = kmeansdata.iloc[:,[3,4,5,6]].values
y_kmeans = kmeansdata.iloc[:,-1].values
y_kmeans

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_kmeans,y_kmeans, test_size=0.4, random_state=1)

kmeans = KMeans(n_clusters=3, init='k-means++')
y_kmeans = kmeans.fit_predict(x_kmeans)
y_kmeans

kmeans.fit(x_train,y_train)
y_pred=knn.predict((x_test))

#confuse matrix
cn = confusion_matrix(y_pred,y_test)
sns.heatmap(cn, annot=True)
plt.xlabel('Predicted values by the classifier')
plt.ylabel('Represents the true values')

print(accuracy_score(y_pred, y_test)*100)
print(classification_report(y_pred,y_test))

print(y_kmeans)

# x_kmeans and y_kmeans are your data and cluster labels, and kmeans in Fraud KMeans model

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Scatter plot for each cluster
for cluster_label in np.unique(y_kmeans):
    ax.scatter(x_kmeans[y_kmeans == cluster_label, 0],
               x_kmeans[y_kmeans == cluster_label, 1],
               x_kmeans[y_kmeans == cluster_label, 2],
               s=5, label=f'Cluster {cluster_label + 1}')

# Scatter plot for cluster centers
ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2],
           s=140, c='black', marker='o', label='Centroids')

# Set labels and title
ax.set_xlabel('X-axis oldbalanceOrg')
ax.set_ylabel('Y-axis newbalanceOrig')
ax.set_zlabel('Z-axis isFlaggedFraud')
ax.set_title('Clusters and Centroids')

# Add legend
ax.legend()

# Show the plot
plt.show()

from sklearn.metrics import silhouette_score

# Assuming 'X' is your data and 'kmeans.labels_' are your cluster labels from K-means
silhouette_avg = silhouette_score(x_kmeans, kmeans.labels_)

print("The average silhouette_score is :", silhouette_avg)

# Case 3 Calculate the time difference between the current transaction and the previous transaction for each account. This new variable could be valuable because fraudulent activities might be associated with sudden changes in transaction patterns or unusual gaps between transactions.

# Create a new variable representing the difference between 'amount' and 'newbalanceOrig'
dataset['amount_newbalance_diff'] = dataset['amount'] - dataset['newbalanceOrig']

# Select features for the model
features = [ 'oldbalanceOrg','amount_newbalance_diff']

# X contains the features, y contains the target variable (isFraud)
X = dataset[features]
y = dataset['isFraud']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create a Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Convert predicted values to binary labels (0 or 1)
y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]

# Display the classification report
classification_rep = classification_report(y_test, y_pred_binary)
print("Classification Report:\n", classification_rep)

accuracy = accuracy_score(y_test, y_pred_binary)
print(f"Accuracy: {accuracy:.2f}")

# Evaluate the model using Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

plt.scatter(y_test, y_pred, color='blue', alpha=0.5)
plt.plot([0, 1], [0, 1], linestyle='-', color='red', linewidth=2)  # Diagonal line for reference
plt.xlabel('Actual isFraud')
plt.ylabel('Predicted isFraud')
plt.title('Linear Regression for Fraud Detection - Scatter Plot')
plt.show()

